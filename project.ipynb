{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Computer Vision - Project\n",
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import requests\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf\n",
    "import h5py, requests, os\n",
    "import matplotlib.patches as patches\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten, BatchNormalization, ZeroPadding2D, GlobalMaxPooling2D\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Resizing\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMG_SIZE = (64, 64)\n",
    "# IMG_SHAPE = IMG_SIZE + (3,)\n",
    "# base_model = tf.keras.applications.resnet(input_shape=IMG_SHAPE,\n",
    "#                                                include_top=False,\n",
    "#                                                weights='imagenet')\n",
    "# base_model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ZIP_PATH = \"https://drive.google.com/drive/folders/1jzHYpTwywUYA53nMGHVROSuVO14hEueq?usp=sharing/\"\n",
    "FILE_NAME =\"SynthText_train.h5\"\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "  tf.config.experimental.set_memory_growth(gpu, True)\n",
    "tf.keras.backend.clear_session()\n",
    "SIZE=227\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_points(points, center):\n",
    "    # calculate the angle of each point from the center point\n",
    "    angles = np.arctan2(points[:, 1] - center[1], points[:, 0] - center[0])\n",
    "    # sort the points by angle\n",
    "    sorted_points = points[np.argsort(angles)]\n",
    "    return sorted_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_training_curve(history):\n",
    "\t\"\"\"\n",
    "\tDraw training curve\n",
    "\tParameters:\n",
    "\t\thistory - contains loss and accuracy from training\n",
    "\tReturns:\n",
    "\t\tNone\n",
    "\t\"\"\"\n",
    "\tplt.figure(1)\n",
    "\n",
    "\t# History for accuracy\n",
    "\tplt.subplot(211)\n",
    "\tplt.plot(history.history['acc'])\n",
    "\tplt.plot(history.history['val_acc'])\n",
    "\tplt.title('model accuracy')\n",
    "\tplt.ylabel('accuracy')\n",
    "\tplt.xlabel('epoch')\n",
    "\tplt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "\t# History for loss\n",
    "\tplt.subplot(212)\n",
    "\tplt.plot(history.history['loss'])\n",
    "\tplt.plot(history.history['val_loss'])\n",
    "\tplt.title('model loss')\n",
    "\tplt.ylabel('loss')\n",
    "\tplt.xlabel('epoch')\n",
    "\tplt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_points(image, points, color=(255, 0, 0), radius=3):\n",
    "    # create a copy of the image\n",
    "    img = image.copy()\n",
    "    # iterate over the points and draw them on the image\n",
    "    for point in points:\n",
    "        cv2.circle(img, tuple(map(int, point)), radius, color, -1)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bb(img, bbs, indx):\n",
    "    x1 = int(bbs[0,0,indx])\n",
    "    y1 = int(bbs[1,0,indx])\n",
    "    x2 = int(bbs[0,1,indx])\n",
    "    y2 = int(bbs[1,1,indx])\n",
    "    x3 = int(bbs[0,2,indx])\n",
    "    y3 = int(bbs[1,2,indx])\n",
    "    x4 = int(bbs[0,3,indx])\n",
    "    y4 = int(bbs[1,3,indx])\n",
    "    # calculate bounding rectangle\n",
    "    top_left_x = max(0, min([x1,x2,x3,x4]))\n",
    "    top_left_y = max(0, min([y1,y2,y3,y4]))\n",
    "    bot_right_x = max(0, max([x1,x2,x3,x4]))\n",
    "    bot_right_y = max(0, max([y1,y2,y3,y4]))\n",
    "    # points = np.float32([[x1, y1], [x2, y2], [x3, y3], [x4, y4]])\n",
    "    \n",
    "    # create an empty image with the same shape as the input image\n",
    "    # mask = np.zeros_like(img[:,:,0])\n",
    "    # create a list of the bounding box points in the correct format\n",
    "    # bounding_box = np.array([points], dtype=np.int32)\n",
    "    # fill the area inside the bounding box with white\n",
    "    # cv2.fillPoly(mask, bounding_box, 255)\n",
    "    # apply the mask to the image\n",
    "    ####res = cv2.bitwise_and(img, mask)\n",
    "    # try1 = np.array(np.where(mask == 255, 255, 0), dtype=np.uint8)\n",
    "    # print(x)\n",
    "    # inv_mask = cv2.bitwise_not(mask)\n",
    "    # res2 = cv2.bitwise_and(img, inv_mask)\n",
    "    # bb2 = np.int32([[top_left_x, bot_right_y], [bot_right_x, bot_right_y],[bot_right_x,top_left_y], [top_left_x,top_left_y]])\n",
    "    # print(bb2)\n",
    "    # mask2 = np.zeros_like(img)\n",
    "    # # bb2 = np.array(frame, dtype=np.int32)\n",
    "    # cv2.fillPoly(mask2, bb2, (255, 255, 255))\n",
    "    # mask = mask[top_left_y:bot_right_y+1, top_left_x:bot_right_x+1]\n",
    "    res = img[top_left_y:bot_right_y+1, top_left_x:bot_right_x+1]\n",
    "\n",
    "    #res = img[top_left_y:bot_right_y+1, top_left_x:bot_right_x+1]\n",
    "    #flipping\n",
    "    \"\"\"\"if(x2 < x1):\n",
    "        res = cv2.flip(res, 1)\n",
    "    if(y2 < y1):\n",
    "        res = cv2.flip(res, 0)\n",
    "    plt.imshow(mask)\n",
    "    plt.show()\n",
    "    print(mask.shape)\"\"\"\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def font_to_num(font):\n",
    "    if font == b'Alex Brush':\n",
    "        return 0\n",
    "    elif font == b'Titillium Web':\n",
    "        return 1\n",
    "    elif font == b'Sansation':\n",
    "        return 2\n",
    "    elif font == b'Open Sans':\n",
    "        return 3\n",
    "    else:\n",
    "        return 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_to_font(font):\n",
    "    if font == 0:\n",
    "        return b'Alex Brush'\n",
    "    elif font == 1:\n",
    "        return b'Titillium Web'\n",
    "    elif font == 2:\n",
    "        return b'Sansation'\n",
    "    elif font == 3:\n",
    "        return b'Open Sans'\n",
    "    else:\n",
    "        return b'Ubuntu Mono'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_label(set, index):\n",
    "    line = set[index]\n",
    "    max = np.argmax(line)\n",
    "    print(num_to_font(max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_font(max):\n",
    "     print(num_to_font(max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_photo_from_set(set_x, set_y, index, font):\n",
    "    plt.imshow(set_x[index], cmap='gray')\n",
    "    plt.show()\n",
    "    print_font(font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_img(img, bbs, index, size):\n",
    "    cropped = get_bb(img, bbs, index)\n",
    "    # _, cropped = cv2.threshold(cropped,127,255,cv2.THRESH_TRIANGLE)\n",
    "    # print(cropped.shape)\n",
    "    cropped = tf.image.resize(cropped, (size, size), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    # cropped = tf.image.rgb_to_grayscale(cropped)\n",
    "    # # print(cropped.shape)\n",
    "    cropped = tf.image.convert_image_dtype(cropped, tf.float32)\n",
    "    # plt.imshow(cropped, cmap='gray')\n",
    "    # plt.show()\n",
    "    return cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def create_dirs():\n",
    "    Path('main_directory').mkdir(parents=True, exist_ok=True)\n",
    "    Path('main_directory/Alex Brush').mkdir(parents=True, exist_ok=True)\n",
    "    Path('main_directory/Titillium Web').mkdir(parents=True, exist_ok=True)\n",
    "    Path('main_directory/Sansation').mkdir(parents=True, exist_ok=True)\n",
    "    Path('main_directory/Open Sans').mkdir(parents=True, exist_ok=True)\n",
    "    Path('main_directory/Ubuntu Mono').mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_set(size: int):\n",
    "    db = h5py.File(FILE_NAME, 'r')\n",
    "    create_dirs()\n",
    "    im_names = list(db['data'].keys())\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    num = 0\n",
    "    for i in tqdm(range(0, len(im_names)-1)):\n",
    "        im = im_names[i]\n",
    "        img  = db['data'][im][:]\n",
    "        fonts = db['data'][im].attrs['font']\n",
    "        txts = db['data'][im].attrs['txt']\n",
    "        charBBs = db['data'][im].attrs['charBB']\n",
    "        wordBBs = db['data'][im].attrs['wordBB']\n",
    "        font_indx = 0 \n",
    "        char_indx = 0\n",
    "        # print(im)\n",
    "        for j in range(0, len(txts)):\n",
    "            cropped = prepare_img(img, wordBBs, j, size)\n",
    "            path = 'main_directory/'+fonts[font_indx].decode('UTF-8')+'/'+im+'_'+str(num)+'.jpg' \n",
    "            # print(path)\n",
    "            tf.keras.utils.save_img(path,cropped)\n",
    "            num+=1\n",
    "            # train_x.append(cropped)\n",
    "            # train_y.append(font_to_num(fonts[font_indx]))\n",
    "            \n",
    "            # plt.imshow(cropped, cmap='gray')\n",
    "            # plt.show()\n",
    "            # print(fonts[font_indx])\n",
    "            for k in range(0, len(txts[j])):\n",
    "                cropped = prepare_img(img, charBBs, char_indx, size)\n",
    "                path = 'main_directory/'+fonts[font_indx].decode('UTF-8')+'/'+im+'_'+str(num)+'.jpg'\n",
    "                # print(path)\n",
    "                tf.keras.utils.save_img(path,cropped)\n",
    "                num+=1\n",
    "                # train_x.append(cropped)\n",
    "                # train_y.append(font_to_num(fonts[font_indx]))\n",
    "                char_indx+=1\n",
    "                # plt.imshow(cropped, cmap='gray')\n",
    "                # plt.show()\n",
    "                # print(fonts[font_indx])\n",
    "            font_indx += len(txts[j])\n",
    "    print(num)\n",
    "    # return train_x, train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from download_h5py_db import download_h5py_db\n",
    "if not Path(\"main_directory\").exists():\n",
    "    download_h5py_db()\n",
    "    get_data_set(SIZE)\n",
    "# train_x, train_y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "def data_augmentation():\n",
    "    datagen =  ImageDataGenerator(\n",
    "        # horizontal_flip=True, rotation_range=10, fill_mode='reflect', \n",
    "        # shear_range=10, vertical_flip=False, channel_shift_range=1.0, brightness_range=(0.2, 0.8),\n",
    "     rescale=1/255,dtype='float32',validation_split=0.3)\n",
    "    # Path('augmented').mkdir(exist_ok=True)\n",
    "    it = datagen.flow_from_directory('main_directory', batch_size=18, subset='training', class_mode='categorical',\n",
    "    #save_to_dir='augmented',\n",
    "     shuffle=True)\n",
    "    val_it = datagen.flow_from_directory('main_directory', batch_size=18, subset='validation', class_mode='categorical',\n",
    "    #save_to_dir='augmented',\n",
    "     shuffle=True)\n",
    "    return it, val_it, datagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26534 images belonging to 5 classes.\n",
      "Found 11368 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "it, val_it, datagen = data_augmentation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Regularizer = l2(0.007)\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Conv2D(96, kernel_size=(11,11), strides= 4,\n",
    "#                         padding= 'valid', activation= 'relu',\n",
    "#                         input_shape= (SIZE,SIZE,3),\n",
    "#                         kernel_initializer= 'he_normal'))\n",
    "# model.add(MaxPooling2D(pool_size=(3,3), strides= (2,2),\n",
    "#                         padding= 'valid', data_format= None))\n",
    "\n",
    "# model.add(Conv2D(256, kernel_size=(5,5), strides= 1,\n",
    "#                 padding= 'same', activation= 'relu',\n",
    "#                 kernel_initializer= 'he_normal'))\n",
    "# model.add(MaxPooling2D(pool_size=(3,3), strides= (2,2),\n",
    "#                         padding= 'valid', data_format= None)) \n",
    "\n",
    "# model.add(Conv2D(384, kernel_size=(3,3), strides= 1,\n",
    "#                 padding= 'same', activation= 'relu',\n",
    "#                 kernel_initializer= 'he_normal'))\n",
    "\n",
    "# model.add(Conv2D(384, kernel_size=(3,3), strides= 1,\n",
    "#                 padding= 'same', activation= 'relu',\n",
    "#                 kernel_initializer= 'he_normal'))\n",
    "\n",
    "# model.add(Conv2D(256, kernel_size=(3,3), strides= 1,\n",
    "#                         padding= 'same', activation= 'relu',\n",
    "#                         kernel_initializer= 'he_normal'))\n",
    "\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "# model.add(MaxPooling2D(pool_size=(3,3), strides= (2,2),\n",
    "#                         padding= 'valid', data_format= None))\n",
    "\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(4096, activation= 'relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(4096, activation= 'relu'))\n",
    "# model.add(Dense(1000, activation= 'relu', activity_regularizer=Regularizer, kernel_regularizer=Regularizer))\n",
    "# model.add(Dense(5, activation= 'softmax', activity_regularizer=Regularizer, kernel_regularizer=Regularizer))\n",
    "\n",
    "# model.compile(optimizer= tf.keras.optimizers.Adam(learning_rate=5e-4, amsgrad=True, epsilon=0.1),\n",
    "#             loss='categorical_crossentropy',\n",
    "#             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.regularizers import l2\n",
    "# # Initialize model\n",
    "# alexnet = Sequential()\n",
    "# l2_reg = 0\n",
    "\n",
    "# # Layer 1\n",
    "# alexnet.add(Conv2D(96, (11, 11), input_shape=(227, 227, 3),strides= 4,\n",
    "# \tpadding='same', kernel_regularizer=l2(l2_reg)))\n",
    "# alexnet.add(BatchNormalization())\n",
    "# alexnet.add(Activation('relu'))\n",
    "# alexnet.add(MaxPooling2D(pool_size=(3, 3),strides= 2,))\n",
    "\n",
    "# # Layer 2\n",
    "# alexnet.add(Conv2D(256, (5, 5), padding=(2,2)))\n",
    "# alexnet.add(BatchNormalization())\n",
    "# alexnet.add(Activation('relu'))\n",
    "# alexnet.add(MaxPooling2D(pool_size=(3, 3),strides= 2,))\n",
    "\n",
    "# # Layer 3\n",
    "# alexnet.add(Conv2D(384, (3, 3), padding='1'))\n",
    "# alexnet.add(BatchNormalization())\n",
    "# alexnet.add(Activation('relu'))\n",
    "\n",
    "# # Layer 4\n",
    "# alexnet.add(Conv2D(384, (3, 3), padding='1'))\n",
    "# alexnet.add(BatchNormalization())\n",
    "# alexnet.add(Activation('relu'))\n",
    "\n",
    "# # Layer 5\n",
    "# alexnet.add(Conv2D(384, (3, 3), padding='1'))\n",
    "# alexnet.add(BatchNormalization())\n",
    "# alexnet.add(Activation('relu'))\n",
    "\n",
    "# # Layer 6\n",
    "# alexnet.add(Conv2D(256, (3, 3), padding='1'))\n",
    "# alexnet.add(BatchNormalization())\n",
    "# alexnet.add(Activation('relu'))\n",
    "# alexnet.add(MaxPooling2D(pool_size=(3, 3),strides= 2,))\n",
    "\n",
    "# # Layer 7\n",
    "# alexnet.add(Flatten())\n",
    "# alexnet.add(Dense(4096))\n",
    "# alexnet.add(BatchNormalization())\n",
    "# alexnet.add(Activation('relu'))\n",
    "# alexnet.add(Dropout(0.5))\n",
    "\n",
    "# # Layer 8\n",
    "# alexnet.add(Dense(4096))\n",
    "# alexnet.add(BatchNormalization())\n",
    "# alexnet.add(Activation('relu'))\n",
    "# alexnet.add(Dropout(0.5))\n",
    "\n",
    "# alexnet.add(Dense(5))\n",
    "# alexnet.add(BatchNormalization())\n",
    "# alexnet.add(Activation('softmax'))\t\n",
    "\n",
    "# model = alexnet\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "# \t\toptimizer=tf.keras.optimizers.rmsprop(lr=0.0001, decay=1e-6),\n",
    "# \t\tmetrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m i, layer \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(resnet_model\u001b[39m.\u001b[39mlayers):\n\u001b[0;32m     10\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 11\u001b[0m         new_weights \u001b[39m=\u001b[39m pretrained_model\u001b[39m.\u001b[39;49mlayers[i]\u001b[39m.\u001b[39;49mget_weights()[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, keepdims\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     12\u001b[0m         resnet_model\u001b[39m.\u001b[39mset_weights([new_weights])\n\u001b[0;32m     13\u001b[0m         layer\u001b[39m.\u001b[39mtrainable \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "\n",
    "num_classes = 5\n",
    "input_size= SIZE\n",
    "\n",
    "pretrained_model = ResNet50(include_top=False,\n",
    "                         pooling='none',\n",
    "                         input_shape=(input_size, input_size, 3),\n",
    "                        weights='imagenet')\n",
    "cfg = pretrained_model.get_config()\n",
    "cfg['layers'][0]['config']['batch_input_shape'] = (None, input_size, input_size, 1)\n",
    "resnet_model = Model.from_config(cfg)\n",
    "for i, layer in enumerate(resnet_model.layers):\n",
    "    if i == 1:\n",
    "        new_weights = pretrained_model.layers[i].get_weights()[0].sum(axis=2, keepdims=True)\n",
    "        resnet_model.set_weights([new_weights])\n",
    "        layer.trainable = False\n",
    "    else: \n",
    "        layer.set_weights(pretrained_model.layers[i].get_weights())\n",
    "        layer.trainable = False\n",
    "\n",
    "x = GlobalMaxPooling2D()(resnet_model.output)\n",
    "x = Dense(2048, activation='relu')(x)\n",
    "x = Dense(2048, activation='relu')(x)\n",
    "outp = Dense(num_classes, activation='softmax')(x)\n",
    "model = Model(resnet_model.input, outp)        \n",
    "    \n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    # if epoch < 5:\n",
    "    #     return lr\n",
    "    # else:\n",
    "    return lr * 0.1 #tf.math.exp(-0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback = tf.keras.callbacks.LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\simon\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\simon\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\simon\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\simon\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 994, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\simon\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1052, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\simon\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\simon\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\simon\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\losses.py\", line 272, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\simon\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\losses.py\", line 1990, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"c:\\Users\\simon\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\backend.py\", line 5529, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, None) and (None, 8, 8, 2048) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# model.fit(X_train, Y_train, batch_size=16, epochs=20, verbose=1)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# datagen.fit(X_train)\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(it, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m , shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m      4\u001b[0m         \u001b[39m#   ,callbacks=[callback]\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m           )\n",
      "File \u001b[1;32mc:\\Users\\simon\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file6d5uhmew.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\simon\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\simon\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\simon\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\simon\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 994, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\simon\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1052, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\simon\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\simon\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\simon\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\losses.py\", line 272, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\simon\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\losses.py\", line 1990, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"c:\\Users\\simon\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\backend.py\", line 5529, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, None) and (None, 8, 8, 2048) are incompatible\n"
     ]
    }
   ],
   "source": [
    "# model.fit(X_train, Y_train, batch_size=16, epochs=20, verbose=1)\n",
    "# datagen.fit(X_train)\n",
    "history = model.fit(it, epochs=10, verbose=1 , shuffle=True\n",
    "        #   ,callbacks=[callback]\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m draw_training_curve(history)\n",
      "Cell \u001b[1;32mIn[5], line 13\u001b[0m, in \u001b[0;36mdraw_training_curve\u001b[1;34m(history)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39m# History for accuracy\u001b[39;00m\n\u001b[0;32m     12\u001b[0m plt\u001b[39m.\u001b[39msubplot(\u001b[39m211\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m plt\u001b[39m.\u001b[39mplot(history\u001b[39m.\u001b[39;49mhistory[\u001b[39m'\u001b[39;49m\u001b[39macc\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m     14\u001b[0m plt\u001b[39m.\u001b[39mplot(history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mval_acc\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     15\u001b[0m plt\u001b[39m.\u001b[39mtitle(\u001b[39m'\u001b[39m\u001b[39mmodel accuracy\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'acc'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_training_curve(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "632/632 [==============================] - 53s 82ms/step - loss: 1.6858 - accuracy: 0.5157\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(val_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 632/632 [==============================] - 150s 236ms/step - loss: 0.8984 - accuracy: 0.6251\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(val_it, verbose=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "017718ff8815c2e28200b0ec15712f9c3f1df2ede3aa8445637a6e1eb80e7347"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
