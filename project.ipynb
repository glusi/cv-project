{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Computer Vision - Project\n",
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import requests\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf\n",
    "import h5py, requests, os\n",
    "import matplotlib.patches as patches\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten, BatchNormalization, ZeroPadding2D, GlobalMaxPooling2D\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Resizing\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ZIP_PATH = \"https://drive.google.com/drive/folders/1jzHYpTwywUYA53nMGHVROSuVO14hEueq?usp=sharing/\"\n",
    "FILE_NAME =\"SynthText_train.h5\"\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "  tf.config.experimental.set_memory_growth(gpu, True)\n",
    "tf.keras.backend.clear_session()\n",
    "SIZE=224\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_training_curve(history):\n",
    "\t\"\"\"\n",
    "\tDraw training curve\n",
    "\tParameters:\n",
    "\t\thistory - contains loss and accuracy from training\n",
    "\tReturns:\n",
    "\t\tNone\n",
    "\t\"\"\"\n",
    "\tplt.figure(1)\n",
    "\n",
    "\t# History for accuracy\n",
    "\tplt.subplot(211)\n",
    "\tplt.plot(history.history['accuracy'])\n",
    "\tplt.plot(history.history['val_accuracy'])\n",
    "\tplt.title('model accuracy')\n",
    "\tplt.ylabel('accuracy')\n",
    "\tplt.xlabel('epoch')\n",
    "\tplt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "\t# History for loss\n",
    "\tplt.subplot(212)\n",
    "\tplt.plot(history.history['loss'])\n",
    "\tplt.plot(history.history['val_loss'])\n",
    "\tplt.title('model loss')\n",
    "\tplt.ylabel('loss')\n",
    "\tplt.xlabel('epoch')\n",
    "\tplt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def font_to_num(font):\n",
    "    if font == b'Alex Brush':\n",
    "        return 0\n",
    "    elif font == b'Open Sans':\n",
    "        return 1\n",
    "    elif font == b'Sansation':\n",
    "        return 2\n",
    "    elif font == b'Titillium Web':\n",
    "        return 3\n",
    "    else:\n",
    "        return 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_to_font(font):\n",
    "    if font == 0:\n",
    "        return b'Alex Brush'\n",
    "    elif font == 1:\n",
    "        return b'Open Sans'\n",
    "    elif font == 2:\n",
    "        return b'Sansation'\n",
    "    elif font == 3:\n",
    "        return b'Titillium Web'\n",
    "    else:\n",
    "        return b'Ubuntu Mono'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_img(img, bbs, index, size):\n",
    "    x1 = int(bbs[0,0,index])\n",
    "    y1 = int(bbs[1,0,index])\n",
    "    x2 = int(bbs[0,1,index])\n",
    "    y2 = int(bbs[1,1,index])\n",
    "    x3 = int(bbs[0,2,index])\n",
    "    y3 = int(bbs[1,2,index])\n",
    "    x4 = int(bbs[0,3,index])\n",
    "    y4 = int(bbs[1,3,index])\n",
    "    # calculate bounding rectangle\n",
    "    top_left_x = max(0, min([x1,x2,x3,x4]))\n",
    "    top_left_y = max(0, min([y1,y2,y3,y4]))\n",
    "    bot_right_x = max(0, max([x1,x2,x3,x4]))\n",
    "    bot_right_y = max(0, max([y1,y2,y3,y4]))\n",
    "\n",
    "    cropped = img[top_left_y:bot_right_y+1, top_left_x:bot_right_x+1]\n",
    "    # _, cropped = cv2.threshold(cropped,127,255,cv2.THRESH_TRIANGLE)\n",
    "    # print(cropped.shape)\n",
    "    cropped = tf.image.resize(cropped, (size, size), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    # cropped = tf.image.rgb_to_grayscale(cropped)\n",
    "    # # print(cropped.shape)\n",
    "    cropped = tf.image.convert_image_dtype(cropped, tf.float32)\n",
    "    # plt.imshow(cropped, cmap='gray')\n",
    "    # plt.show()\n",
    "    return cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def create_dirs(main_directory):\n",
    "    Path(main_directory).mkdir(parents=True, exist_ok=True)\n",
    "    Path(main_directory+'/Alex Brush').mkdir(parents=True, exist_ok=True)\n",
    "    Path(main_directory+'/Titillium Web').mkdir(parents=True, exist_ok=True)\n",
    "    Path(main_directory+'/Sansation').mkdir(parents=True, exist_ok=True)\n",
    "    Path(main_directory+'/Open Sans').mkdir(parents=True, exist_ok=True)\n",
    "    Path(main_directory+'/Ubuntu Mono').mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_data(db, im):\n",
    "    img  = db['data'][im][:]\n",
    "    fonts = db['data'][im].attrs['font']\n",
    "    txts = db['data'][im].attrs['txt']\n",
    "    charBBs = db['data'][im].attrs['charBB']\n",
    "    wordBBs = db['data'][im].attrs['wordBB']\n",
    "    return img, fonts, txts, charBBs, wordBBs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_num_or_letter(inp):\n",
    "    # res= ((inp >= ord('a') and inp <= ord('z')) or (inp >= ord('A') and inp <= ord('Z'))) or (inp>=ord('0') and inp<=ord('9'))\n",
    "    # # print(chr(inp)+\"=\"+str(res))\n",
    "    if(inp==ord('.') or inp==ord(':')):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_and_save(img, BBs, indx, size, curr_font, im, num, do_save=False, folder='main_directory/'):\n",
    "    cropped = prepare_img(img, BBs, indx, size)\n",
    "    path = folder+curr_font.decode('UTF-8')+'/'+im+'_'+str(num)+'.jpg' \n",
    "    if do_save:\n",
    "        tf.keras.utils.save_img(path,cropped)\n",
    "    # print(curr_font)\n",
    "    # plt.imshow(cropped)\n",
    "    # plt.show()\n",
    "    return cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = h5py.File(FILE_NAME, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_set(db, size: int):   \n",
    "    create_dirs('main_directory')\n",
    "    im_names = list(db['data'].keys())\n",
    "    num = 0\n",
    "    for i in tqdm(range(0, len(im_names)-1)):\n",
    "        im = im_names[i]\n",
    "        img, fonts, txts, charBBs, wordBBs = get_image_data(db, im)\n",
    "        font_indx = 0 \n",
    "        char_indx = 0\n",
    "        # print(im)\n",
    "        for j in range(0, len(txts)):\n",
    "            if(j%8!=0):\n",
    "                curr_font = fonts[font_indx]\n",
    "                crop_and_save(img, wordBBs, j, size, curr_font, im, num, True)\n",
    "                num+=1\n",
    "                for k in range(0, len(txts[j])):\n",
    "                    if(is_num_or_letter(txts[j][k])):\n",
    "                        crop_and_save(img, charBBs, char_indx, size, curr_font, im, num, True)\n",
    "                        num+=1\n",
    "                    char_indx+=1\n",
    "            else:\n",
    "                char_indx+=len(txts[j])\n",
    "            font_indx += len(txts[j])\n",
    "    print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from download_h5py_db import download_h5py_db\n",
    "if not Path(\"main_directory\").exists():\n",
    "    download_h5py_db()\n",
    "    get_data_set(db, SIZE)\n",
    "# train_x, train_y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import splitfolders\n",
    "if not Path(\"main_directory_splitted\").exists():\n",
    "    splitfolders.ratio(\"main_directory/\", # The location of dataset\n",
    "                   output=\"main_directory_splitted\", # The output location\n",
    "                   seed=42, # The number of seed\n",
    "                   ratio=(.8, .1, .1), # The ratio of splited dataset\n",
    "                   group_prefix=None, # If your dataset contains more than one file like \".jpg\", \".pdf\", etc\n",
    "                   move=False # If you choose to move, turn this into True\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "def data_augmentation():\n",
    "    datagen =  ImageDataGenerator(\n",
    "        horizontal_flip=True, rotation_range=90, fill_mode='reflect', channel_shift_range=0.8,#\n",
    "         shear_range=15,vertical_flip=False, brightness_range=(0.2, 0.8),# \n",
    "     rescale=1/255, dtype='float32'\n",
    "     #,validation_split=0.25\n",
    "     )\n",
    "    # Path('augmented').mkdir(exist_ok=True)\n",
    "    it = datagen.flow_from_directory('main_directory_splitted/train/', batch_size=18, class_mode='categorical',\n",
    "    #save_to_dir='augmented',\n",
    "     shuffle=True, seed=1, keep_aspect_ratio=True)\n",
    "    datagen_val =  ImageDataGenerator(\n",
    "     rescale=1/255, dtype='float32'\n",
    "     )\n",
    "    val_it = datagen_val.flow_from_directory('main_directory_splitted/val/', batch_size=18, class_mode='categorical',\n",
    "    #save_to_dir='augmented',\n",
    "     shuffle=True)\n",
    "    test_it = datagen_val.flow_from_directory('main_directory_splitted/test/', batch_size=18, class_mode='categorical',\n",
    "    #save_to_dir='augmented',\n",
    "     shuffle=True)\n",
    "    return it, val_it,test_it, datagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24045 images belonging to 5 classes.\n",
      "Found 3003 images belonging to 5 classes.\n",
      "Found 3010 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "it, val_it,test_it, datagen = data_augmentation()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Found 27011 images belonging to 5 classes.\n",
    "Found 9000 images belonging to 5 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import GaussianNoise\n",
    "from keras import regularizers\n",
    "\n",
    "num_classes = 5\n",
    "input_size= SIZE\n",
    "\n",
    "baseModel = tf.keras.applications.ResNet50(include_top=False, classes=num_classes,\n",
    "                         input_shape=(input_size, input_size, 3),\n",
    "                        weights='imagenet')\n",
    "headModel = baseModel.output\n",
    "headModel = GaussianNoise(0.1)(headModel)\n",
    "headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n",
    "headModel = Flatten(name=\"flatten\")(headModel)\n",
    "headModel = Dropout(0.5)(headModel)\n",
    "headModel = Dense(256, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01))(headModel)\n",
    "headModel = Dropout(0.5)(headModel)\n",
    "headModel = Dense(num_classes, activation=\"softmax\")(headModel)\n",
    "model = Model(inputs=baseModel.input, outputs=headModel)    \n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.00005), loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import callbacks\n",
    "early_stopping=callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='min')\n",
    "\n",
    "filepath=\"res/top_model.h5\"\n",
    "\n",
    "checkpoint = callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "callbacks_list = [early_stopping,checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1336/1336 [==============================] - ETA: 0s - loss: 5.1323 - accuracy: 0.4195\n",
      "Epoch 1: val_loss improved from inf to 3.97772, saving model to res\\top_model.h5\n",
      "1336/1336 [==============================] - 436s 322ms/step - loss: 5.1323 - accuracy: 0.4195 - val_loss: 3.9777 - val_accuracy: 0.5305\n",
      "Epoch 2/25\n",
      " 201/1336 [===>..........................] - ETA: 6:11 - loss: 3.8359 - accuracy: 0.5340"
     ]
    }
   ],
   "source": [
    "# model.fit(X_train, Y_train, batch_size=16, epochs=20)\n",
    "# datagen.fit(X_train)\n",
    "# if not os.path.isfile('model_res.h5'):\n",
    "history = model.fit(it, epochs=25, shuffle=True, validation_data=val_it, verbose=1\n",
    "            ,callbacks=callbacks_list\n",
    "            )\n",
    "# else:\n",
    "#   model = tf.keras.models.load_model('model_res.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167/167 [==============================] - 13s 71ms/step - loss: 0.4731 - accuracy: 0.8368\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.4731270968914032 / Test accuracy: 0.8367893099784851\n"
     ]
    }
   ],
   "source": [
    "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[158], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m draw_training_curve(history)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "draw_training_curve(history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_res\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_res\\assets\n"
     ]
    }
   ],
   "source": [
    "# model.save('model_res')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.load_model('model_res_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 249/249 [00:23<00:00, 10.52it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "create_dirs('test_set')\n",
    "size=SIZE\n",
    "im_names = list(db['data'].keys())\n",
    "num = 0\n",
    "prediction_arr=[]\n",
    "test_y = []\n",
    "\n",
    "for i in tqdm(range(0, int((len(im_names)-1)/4))):\n",
    "    im = im_names[i]\n",
    "    img, fonts, txts, charBBs, wordBBs = get_image_data(db, im)\n",
    "    font_indx = 0 \n",
    "    char_indx = 0\n",
    "    \n",
    "    # print(im)\n",
    "    for j in range(0, int(len(txts)/4)):\n",
    "        if(j%8==0): \n",
    "            curr_font = fonts[font_indx]\n",
    "            test_x = [] \n",
    "            cropped = crop_and_save(img, wordBBs, j, size, curr_font, im, num)\n",
    "            test_x.append(cropped)\n",
    "            # test_y.append(font_to_num(curr_font))\n",
    "            num+=1            \n",
    "            for k in range(0, len(txts[j])):\n",
    "                # if(is_num_or_letter(txts[j][k])):\n",
    "                cropped = crop_and_save(img, charBBs, char_indx, size, curr_font, im, num)\n",
    "                test_x.append(cropped)\n",
    "                    # test_y.append(font_to_num(curr_font))\n",
    "                num+=1\n",
    "                char_indx+=1\n",
    "            test_x = np.asarray(test_x, dtype=np.float32)\n",
    "            # # print(test_x)\n",
    "            reses = model.predict(test_x, verbose=0)\n",
    "            # # print(reses)\n",
    "            maxes = np.argmax(reses, axis=1)\n",
    "            # print(np.bincount(maxes))\n",
    "            prediction = np.bincount(maxes)\n",
    "            prediction = np.argwhere(prediction==prediction.max())\n",
    "            if (len(prediction)>1):\n",
    "                # sum_p = reses.sum(axis=0)\n",
    "                prediction = sum_p.argmax()\n",
    "\n",
    "                if(font_to_num(curr_font)!=prediction):\n",
    "                    # print(sum_p)\n",
    "                    reses_n = normalize(reses, axis=1, norm='l1')\n",
    "                    maxes_n = np.argmax(reses_n, axis=1)\n",
    "                    # print(np.bincount(maxes))\n",
    "                    prediction = np.bincount(maxes_n)\n",
    "                    prediction = np.argmax(prediction)\n",
    "                    if(font_to_num(curr_font)!=prediction):\n",
    "                        sum_p = np.sum(reses, axis=0)\n",
    "                        prediction = sum_p.argmax()\n",
    "                    # # print(reses[0]/np.sum(reses, axis=1)[0])\n",
    "                    #     print(maxes)\n",
    "                    # print(prediction)\n",
    "                    # plt.imshow(test_x[0])\n",
    "                    # plt.show()\n",
    "                    # print(prediction)\n",
    "                    # print(font_to_num(fonts[font_indx]))\n",
    "            for k in range(0, len(txts[j])):\n",
    "                test_y.append(font_to_num(curr_font))\n",
    "                prediction_arr.append(prediction.item())\n",
    "                # print(prediction.item())\n",
    "            # if(font_to_num(curr_font)!=prediction):\n",
    "            #     plt.imshow(test_x[0])\n",
    "            #     plt.show()\n",
    "            #     print(maxes)\n",
    "            #     print(reses)\n",
    "            #     print(prediction)\n",
    "            #     print(font_to_num(fonts[font_indx]))\n",
    "            #     print('----------------------')\n",
    "            # print(num_to_font(prediction))\n",
    "            # print(test_x)\n",
    "            # print(str(np.bincount(maxes).argmax())+\"-\"+str(font_to_num(fonts[font_indx])))\n",
    "            # print(np.bincount(maxes).argmax())\n",
    "        else:\n",
    "            char_indx+=len(txts[j])\n",
    "        font_indx += len(txts[j])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "837\n",
      "837\n"
     ]
    }
   ],
   "source": [
    "print(len(prediction_arr))\n",
    "print(len(test_y))\n",
    "# test_x = np.asarray(test_x, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred=np.argmax(prediction_arr, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Alex Brush       0.93      1.00      0.97       169\n",
      "    Open Sans       0.80      0.84      0.82       142\n",
      "Titillium Web       0.86      0.89      0.87       208\n",
      "    Sansation       0.89      0.82      0.86       158\n",
      "  Ubuntu Mono       0.93      0.84      0.89       160\n",
      "\n",
      "     accuracy                           0.88       837\n",
      "    macro avg       0.88      0.88      0.88       837\n",
      " weighted avg       0.88      0.88      0.88       837\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "labels=['Alex Brush','Open Sans','Titillium Web','Sansation','Ubuntu Mono']\n",
    "print(classification_report(test_y, prediction_arr, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1003\n"
     ]
    }
   ],
   "source": [
    "print(len(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 4s 105ms/step\n"
     ]
    }
   ],
   "source": [
    "prediction_arr = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=np.argmax(prediction_arr, axis=1)\n",
    "# y_test=np.argmax(test_y_cat, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = np.asarray(test_y, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_cat = np_utils.to_categorical(test_y, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1003,)\n"
     ]
    }
   ],
   "source": [
    "# print(test_y)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "labels=['Alex Brush','Open Sans','Titillium Web','Sansation','Ubuntu Mono']\n",
    "print(classification_report(test_y, prediction_arr, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for im in test_x:\n",
    "#     tf.keras.utils.save_img('path',cropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen_try =  ImageDataGenerator(\n",
    "     rescale=1/255, dtype='float32',\n",
    "     )\n",
    "it_try = datagen_try.flow(test_x,y_pred, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 3s 107ms/step\n"
     ]
    }
   ],
   "source": [
    "pted_try = model.predict(it_try)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=np.argmax(prediction_arr, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Alex Brush       0.80      0.93      0.86       204\n",
      "Titillium Web       0.14      0.12      0.13       195\n",
      "    Sansation       0.73      0.68      0.70       248\n",
      "    Open Sans       0.13      0.14      0.13       168\n",
      "  Ubuntu Mono       0.81      0.82      0.81       188\n",
      "\n",
      "     accuracy                           0.56      1003\n",
      "    macro avg       0.52      0.54      0.53      1003\n",
      " weighted avg       0.54      0.56      0.55      1003\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_y, y_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3850 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen_t =  ImageDataGenerator(\n",
    "     rescale=1/255, dtype='float32'\n",
    "     )\n",
    "t_it = datagen_t.flow_from_directory('check_test', batch_size=18, class_mode='categorical',shuffle=False\n",
    "#save_to_dir='augmented',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214/214 [==============================] - 15s 68ms/step - loss: 0.6127 - accuracy: 0.7730\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(t_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214/214 [==============================] - 14s 64ms/step\n"
     ]
    }
   ],
   "source": [
    "predict = model.predict(t_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the class with highest probability for each sample\n",
    "y_pred = np.argmax(predict, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Alex Brush       0.88      0.91      0.90       818\n",
      "Titillium Web       0.55      0.60      0.57       547\n",
      "    Sansation       0.74      0.72      0.73       910\n",
      "    Open Sans       0.75      0.61      0.68       732\n",
      "  Ubuntu Mono       0.86      0.95      0.90       843\n",
      "\n",
      "     accuracy                           0.77      3850\n",
      "    macro avg       0.76      0.76      0.76      3850\n",
      " weighted avg       0.77      0.77      0.77      3850\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(t_it.classes, y_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction_arr_cat = np_utils.to_categorical(prediction_arr, 5)\n",
    "# test_y_cat = np_utils.to_categorical(test_y, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# m = tf.keras.metrics.AUC()\n",
    "\n",
    "# m.update_state(test_y, prediction_arr) # assuming both have shape (N,)\n",
    "\n",
    "# r = m.result().numpy()\n",
    "\n",
    "# print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true = test_y_cat\n",
    "# y_pred = prediction_arr\n",
    "# kacc = tf.keras.metrics.Accuracy()\n",
    "# _ = kacc.update_state(y_true, y_pred)\n",
    "# print(f'Keras Accuracy acc: {kacc.result().numpy()*100:.3}')\n",
    "\n",
    "# kbacc = tf.keras.metrics.BinaryAccuracy()\n",
    "# _ = kbacc.update_state(y_true, y_pred)\n",
    "# print(f'Keras BinaryAccuracy acc: {kbacc.result().numpy()*100:.3}')\n",
    "\n",
    "# print(f'SkLearn acc: {accuracy_score(y_true, y_pred)*100:.3}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "017718ff8815c2e28200b0ec15712f9c3f1df2ede3aa8445637a6e1eb80e7347"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
