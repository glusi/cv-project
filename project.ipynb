{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Computer Vision - Project\n",
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import requests\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf\n",
    "import h5py, requests, os\n",
    "import matplotlib.patches as patches\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten, BatchNormalization, ZeroPadding2D, GlobalMaxPooling2D\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Resizing\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ZIP_PATH = \"https://drive.google.com/drive/folders/1jzHYpTwywUYA53nMGHVROSuVO14hEueq?usp=sharing/\"\n",
    "FILE_NAME =\"SynthText_train.h5\"\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "  tf.config.experimental.set_memory_growth(gpu, True)\n",
    "tf.keras.backend.clear_session()\n",
    "SIZE=224\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_points(points, center):\n",
    "    # calculate the angle of each point from the center point\n",
    "    angles = np.arctan2(points[:, 1] - center[1], points[:, 0] - center[0])\n",
    "    # sort the points by angle\n",
    "    sorted_points = points[np.argsort(angles)]\n",
    "    return sorted_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_training_curve(history):\n",
    "\t\"\"\"\n",
    "\tDraw training curve\n",
    "\tParameters:\n",
    "\t\thistory - contains loss and accuracy from training\n",
    "\tReturns:\n",
    "\t\tNone\n",
    "\t\"\"\"\n",
    "\tplt.figure(1)\n",
    "\n",
    "\t# History for accuracy\n",
    "\tplt.subplot(211)\n",
    "\tplt.plot(history.history['accuracy'])\n",
    "\tplt.plot(history.history['val_accuracy'])\n",
    "\tplt.title('model accuracy')\n",
    "\tplt.ylabel('accuracy')\n",
    "\tplt.xlabel('epoch')\n",
    "\tplt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "\t# History for loss\n",
    "\tplt.subplot(212)\n",
    "\tplt.plot(history.history['loss'])\n",
    "\tplt.plot(history.history['val_loss'])\n",
    "\tplt.title('model loss')\n",
    "\tplt.ylabel('loss')\n",
    "\tplt.xlabel('epoch')\n",
    "\tplt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_points(image, points, color=(255, 0, 0), radius=3):\n",
    "    # create a copy of the image\n",
    "    img = image.copy()\n",
    "    # iterate over the points and draw them on the image\n",
    "    for point in points:\n",
    "        cv2.circle(img, tuple(map(int, point)), radius, color, -1)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def font_to_num(font):\n",
    "    if font == b'Alex Brush':\n",
    "        return 0\n",
    "    elif font == b'Titillium Web':\n",
    "        return 1\n",
    "    elif font == b'Sansation':\n",
    "        return 2\n",
    "    elif font == b'Open Sans':\n",
    "        return 3\n",
    "    else:\n",
    "        return 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_to_font(font):\n",
    "    if font == 0:\n",
    "        return b'Alex Brush'\n",
    "    elif font == 1:\n",
    "        return b'Titillium Web'\n",
    "    elif font == 2:\n",
    "        return b'Sansation'\n",
    "    elif font == 3:\n",
    "        return b'Open Sans'\n",
    "    else:\n",
    "        return b'Ubuntu Mono'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_label(set, index):\n",
    "    line = set[index]\n",
    "    max = np.argmax(line)\n",
    "    print(num_to_font(max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_font(max):\n",
    "     print(num_to_font(max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_photo_from_set(set_x, set_y, index, font):\n",
    "    plt.imshow(set_x[index], cmap='gray')\n",
    "    plt.show()\n",
    "    print_font(font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_img(img, bbs, index, size):\n",
    "    x1 = int(bbs[0,0,index])\n",
    "    y1 = int(bbs[1,0,index])\n",
    "    x2 = int(bbs[0,1,index])\n",
    "    y2 = int(bbs[1,1,index])\n",
    "    x3 = int(bbs[0,2,index])\n",
    "    y3 = int(bbs[1,2,index])\n",
    "    x4 = int(bbs[0,3,index])\n",
    "    y4 = int(bbs[1,3,index])\n",
    "    # calculate bounding rectangle\n",
    "    top_left_x = max(0, min([x1,x2,x3,x4]))\n",
    "    top_left_y = max(0, min([y1,y2,y3,y4]))\n",
    "    bot_right_x = max(0, max([x1,x2,x3,x4]))\n",
    "    bot_right_y = max(0, max([y1,y2,y3,y4]))\n",
    "\n",
    "    cropped = img[top_left_y:bot_right_y+1, top_left_x:bot_right_x+1]\n",
    "    # _, cropped = cv2.threshold(cropped,127,255,cv2.THRESH_TRIANGLE)\n",
    "    # print(cropped.shape)\n",
    "    cropped = tf.image.resize(cropped, (size, size), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    # cropped = tf.image.rgb_to_grayscale(cropped)\n",
    "    # # print(cropped.shape)\n",
    "    cropped = tf.image.convert_image_dtype(cropped, tf.float32)\n",
    "    # plt.imshow(cropped, cmap='gray')\n",
    "    # plt.show()\n",
    "    return cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def create_dirs(main_directory):\n",
    "    Path(main_directory).mkdir(parents=True, exist_ok=True)\n",
    "    Path(main_directory+'/Alex Brush').mkdir(parents=True, exist_ok=True)\n",
    "    Path(main_directory+'/Titillium Web').mkdir(parents=True, exist_ok=True)\n",
    "    Path(main_directory+'/Sansation').mkdir(parents=True, exist_ok=True)\n",
    "    Path(main_directory+'/Open Sans').mkdir(parents=True, exist_ok=True)\n",
    "    Path(main_directory+'/Ubuntu Mono').mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_data(db, im):\n",
    "    img  = db['data'][im][:]\n",
    "    fonts = db['data'][im].attrs['font']\n",
    "    txts = db['data'][im].attrs['txt']\n",
    "    charBBs = db['data'][im].attrs['charBB']\n",
    "    wordBBs = db['data'][im].attrs['wordBB']\n",
    "    return img, fonts, txts, charBBs, wordBBs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_num_or_letter(inp):\n",
    "    # res= ((inp >= ord('a') and inp <= ord('z')) or (inp >= ord('A') and inp <= ord('Z'))) or (inp>=ord('0') and inp<=ord('9'))\n",
    "    # # print(chr(inp)+\"=\"+str(res))\n",
    "    if(inp==ord('.') or inp==ord(',') or inp==ord(':') or inp==ord(';')):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_and_save(img, BBs, indx, size, curr_font, im, num, append_not_save=False):\n",
    "    cropped = prepare_img(img, BBs, indx, size)\n",
    "    path = 'main_directory/'+curr_font.decode('UTF-8')+'/'+im+'_'+str(num)+'.jpg' \n",
    "    if not append_not_save:\n",
    "        tf.keras.utils.save_img(path,cropped)\n",
    "    return cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = h5py.File(FILE_NAME, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_set(db, size: int):   \n",
    "    create_dirs('main_directory')\n",
    "    im_names = list(db['data'].keys())\n",
    "    num = 0\n",
    "    for i in tqdm(range(0, len(im_names)-1)):\n",
    "        im = im_names[i]\n",
    "        img, fonts, txts, charBBs, wordBBs = get_image_data(db, im)\n",
    "        font_indx = 0 \n",
    "        char_indx = 0\n",
    "        # print(im)\n",
    "        for j in range(0, len(txts)):\n",
    "            if(j%8!=0):\n",
    "                crop_and_save(img, wordBBs, j, size, fonts, font_indx, im, num)\n",
    "                num+=1\n",
    "                for k in range(0, len(txts[j])):\n",
    "                    if(is_num_or_letter(txts[j][k])):\n",
    "                        crop_and_save(img, charBBs, char_indx, size, fonts, font_indx, im, num)\n",
    "                        num+=1\n",
    "                    char_indx+=1\n",
    "            else:\n",
    "                char_indx+=len(txts[j])\n",
    "            font_indx += len(txts[j])\n",
    "    print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from download_h5py_db import download_h5py_db\n",
    "if not Path(\"main_directory\").exists():\n",
    "    download_h5py_db()\n",
    "    get_data_set(db, SIZE)\n",
    "# train_x, train_y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 29840 files [00:51, 584.48 files/s]\n"
     ]
    }
   ],
   "source": [
    "import splitfolders\n",
    "splitfolders.ratio(\"main_directory/\", # The location of dataset\n",
    "                   output=\"main_directory_splitted\", # The output location\n",
    "                   seed=42, # The number of seed\n",
    "                   ratio=(.8, .1, .1), # The ratio of splited dataset\n",
    "                   group_prefix=None, # If your dataset contains more than one file like \".jpg\", \".pdf\", etc\n",
    "                   move=False # If you choose to move, turn this into True\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "def data_augmentation():\n",
    "    datagen =  ImageDataGenerator(\n",
    "        horizontal_flip=True, rotation_range=90, fill_mode='reflect', channel_shift_range=0.8,#\n",
    "         shear_range=15,vertical_flip=False, brightness_range=(0.2, 0.8),# \n",
    "     rescale=1/255, dtype='float32'\n",
    "     #,validation_split=0.25\n",
    "     )\n",
    "    # Path('augmented').mkdir(exist_ok=True)\n",
    "    it = datagen.flow_from_directory('main_directory_splitted/train/', batch_size=18, class_mode='categorical',\n",
    "    #save_to_dir='augmented',\n",
    "     shuffle=True, seed=1, keep_aspect_ratio=True)\n",
    "    datagen_val =  ImageDataGenerator(\n",
    "     rescale=1/255, dtype='float32'\n",
    "     )\n",
    "    val_it = datagen_val.flow_from_directory('main_directory_splitted/val/', batch_size=18, class_mode='categorical',\n",
    "    #save_to_dir='augmented',\n",
    "     shuffle=True)\n",
    "    test_it = datagen_val.flow_from_directory('main_directory_splitted/test/', batch_size=18, class_mode='categorical',\n",
    "    #save_to_dir='augmented',\n",
    "     shuffle=True)\n",
    "    return it, val_it,test_it, datagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23869 images belonging to 5 classes.\n",
      "Found 2981 images belonging to 5 classes.\n",
      "Found 2990 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "it, val_it,test_it, datagen = data_augmentation()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Found 27011 images belonging to 5 classes.\n",
    "Found 9000 images belonging to 5 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datagen.fit(it, augment=True, seed=0.8, rounds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import GaussianNoise\n",
    "\n",
    "from keras import regularizers\n",
    "\n",
    "num_classes = 5\n",
    "input_size= SIZE\n",
    "\n",
    "baseModel = tf.keras.applications.ResNet50(include_top=False, classes=num_classes,\n",
    "                         input_shape=(input_size, input_size, 3),\n",
    "                        weights='imagenet')\n",
    "headModel = baseModel.output\n",
    "headModel = GaussianNoise(0.1)(headModel)\n",
    "headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n",
    "headModel = Flatten(name=\"flatten\")(headModel)\n",
    "headModel = Dropout(0.5)(headModel)\n",
    "headModel = Dense(256, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01))(headModel)\n",
    "headModel = Dropout(0.5)(headModel)\n",
    "headModel = Dense(num_classes, activation=\"softmax\")(headModel)\n",
    "model = Model(inputs=baseModel.input, outputs=headModel)    \n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.00003), loss='categorical_crossentropy', metrics=[tf.keras.metrics.Precision()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    # if epoch < 5:\n",
    "    #     return lr\n",
    "    # else:\n",
    "    return lr * 0.1 #tf.math.exp(-0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback = tf.keras.callbacks.LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(X_train, Y_train, batch_size=16, epochs=20)\n",
    "# datagen.fit(X_train)\n",
    "if not os.path.isfile('model_res/saved_model.pb'):\n",
    "  history = model.fit(it, epochs=15, shuffle=True, validation_data=val_it, verbose=1\n",
    "          #   ,callbacks=[callback]\n",
    "            )\n",
    "else:\n",
    "  model = tf.keras.models.load_model('model_res')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167/167 [==============================] - 17s 74ms/step - loss: 0.4731 - accuracy: 0.8368\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.47312721610069275 / Test accuracy: 0.8367893099784851\n"
     ]
    }
   ],
   "source": [
    "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m draw_training_curve(history)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "draw_training_curve(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_res\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_res\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('model_res')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 249/249 [00:02<00:00, 98.93it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1003\n",
      "1003\n"
     ]
    }
   ],
   "source": [
    "size=SIZE\n",
    "im_names = list(db['data'].keys())\n",
    "num = 0\n",
    "prediction_arr=[]\n",
    "test_y = []\n",
    "test_x = []\n",
    "for i in tqdm(range(0, int((len(im_names)-1)/4))):\n",
    "    im = im_names[i]\n",
    "    img, fonts, txts, charBBs, wordBBs = get_image_data(db, im)\n",
    "    font_indx = 0 \n",
    "    char_indx = 0\n",
    "    curr_font = fonts[font_indx]\n",
    "    # print(im)\n",
    "    for j in range(0, int(len(txts)/4)):\n",
    "        if(j%8==0):  \n",
    "            cropped = crop_and_save(img, wordBBs, j, size, curr_font, im, num, True)\n",
    "            test_x.append(cropped)\n",
    "            test_y.append(font_to_num(curr_font))\n",
    "            num+=1            \n",
    "            for k in range(0, len(txts[j])):\n",
    "                if(is_num_or_letter(txts[j][k])):\n",
    "                    cropped = crop_and_save(img, charBBs, char_indx, size, curr_font, im, num, True)\n",
    "                    test_x.append(cropped)\n",
    "                    test_y.append(font_to_num(curr_font))\n",
    "                    num+=1\n",
    "                char_indx+=1\n",
    "            # test_x = np.asarray(test_x, dtype=np.float32)\n",
    "            # # print(test_x)\n",
    "            # reses = model.predict(test_x)\n",
    "            # # print(reses)\n",
    "            # maxes = np.argmax(reses, axis=1)\n",
    "            # prediction = np.bincount(maxes).argmax()\n",
    "            # for k in range(0, len(txts[j])):\n",
    "            #     test_y.append(font_to_num(fonts[font_indx]))\n",
    "            #     prediction_arr.append(prediction)\n",
    "            # if(fonts[font_indx]==b'Open Sans'):\n",
    "            # print(txts[j])\n",
    "            # for imi in test_x:\n",
    "            #     plt.imshow(imi)\n",
    "            #     plt.show()\n",
    "            # print(maxes)\n",
    "            # print(reses)\n",
    "            # # print(prediction)\n",
    "            # print(fonts[font_indx])\n",
    "            # print(num_to_font(prediction))\n",
    "            # print(test_x)\n",
    "            # print(str(np.bincount(maxes).argmax())+\"-\"+str(font_to_num(fonts[font_indx])))\n",
    "            # print(np.bincount(maxes).argmax())\n",
    "        else:\n",
    "            char_indx+=len(txts[j])\n",
    "        font_indx += len(txts[j])\n",
    "print(len(test_x))\n",
    "print(len(test_y))\n",
    "test_x = np.asarray(test_x, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1003\n"
     ]
    }
   ],
   "source": [
    "print(len(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 3s 98ms/step\n"
     ]
    }
   ],
   "source": [
    "prediction_arr = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[106], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y_pred\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39;49margmax(prediction_arr, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m      2\u001b[0m \u001b[39m# y_test=np.argmax(test_y_cat, axis=1)\u001b[39;00m\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\simon\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\fromnumeric.py:1242\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   1155\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1156\u001b[0m \u001b[39mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[0;32m   1157\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1239\u001b[0m \u001b[39m(2, 1, 4)\u001b[39;00m\n\u001b[0;32m   1240\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1241\u001b[0m kwds \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mkeepdims\u001b[39m\u001b[39m'\u001b[39m: keepdims} \u001b[39mif\u001b[39;00m keepdims \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39m_NoValue \u001b[39melse\u001b[39;00m {}\n\u001b[1;32m-> 1242\u001b[0m \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39m\u001b[39margmax\u001b[39m\u001b[39m'\u001b[39m, axis\u001b[39m=\u001b[39maxis, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Users\\simon\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\fromnumeric.py:54\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     52\u001b[0m bound \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, method, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m     53\u001b[0m \u001b[39mif\u001b[39;00m bound \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Users\\simon\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\fromnumeric.py:43\u001b[0m, in \u001b[0;36m_wrapit\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m     wrap \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(asarray(obj), method)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     44\u001b[0m \u001b[39mif\u001b[39;00m wrap:\n\u001b[0;32m     45\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(result, mu\u001b[39m.\u001b[39mndarray):\n",
      "\u001b[1;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "y_pred=np.argmax(prediction_arr, axis=1)\n",
    "# y_test=np.argmax(test_y_cat, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = np.asarray(test_y, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_cat = np_utils.to_categorical(test_y, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1003, 5)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[105], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mprint\u001b[39m(test_y_cat\u001b[39m.\u001b[39mshape)\n\u001b[1;32m----> 2\u001b[0m \u001b[39mprint\u001b[39m(prediction_arr\u001b[39m.\u001b[39;49mshape)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "print(test_y_cat.shape)\n",
    "print(prediction_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1003, 0]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[108], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m classification_report\n\u001b[0;32m      2\u001b[0m labels\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mAlex Brush\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mTitillium Web\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mSansation\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mOpen Sans\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mUbuntu Mono\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m----> 3\u001b[0m \u001b[39mprint\u001b[39m(classification_report(test_y_cat, prediction_arr, target_names\u001b[39m=\u001b[39;49mlabels))\n",
      "File \u001b[1;32mc:\\Users\\simon\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2310\u001b[0m, in \u001b[0;36mclassification_report\u001b[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[0;32m   2195\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclassification_report\u001b[39m(\n\u001b[0;32m   2196\u001b[0m     y_true,\n\u001b[0;32m   2197\u001b[0m     y_pred,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2204\u001b[0m     zero_division\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwarn\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   2205\u001b[0m ):\n\u001b[0;32m   2206\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build a text report showing the main classification metrics.\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m \n\u001b[0;32m   2208\u001b[0m \u001b[39m    Read more in the :ref:`User Guide <classification_report>`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2307\u001b[0m \u001b[39m    <BLANKLINE>\u001b[39;00m\n\u001b[0;32m   2308\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2310\u001b[0m     y_type, y_true, y_pred \u001b[39m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[0;32m   2312\u001b[0m     \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2313\u001b[0m         labels \u001b[39m=\u001b[39m unique_labels(y_true, y_pred)\n",
      "File \u001b[1;32mc:\\Users\\simon\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:86\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[0;32m     60\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \n\u001b[0;32m     62\u001b[0m \u001b[39m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[39m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m     check_consistent_length(y_true, y_pred)\n\u001b[0;32m     87\u001b[0m     type_true \u001b[39m=\u001b[39m type_of_target(y_true, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my_true\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     88\u001b[0m     type_pred \u001b[39m=\u001b[39m type_of_target(y_pred, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my_pred\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\simon\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    395\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[0;32m    396\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 397\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    398\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    399\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[0;32m    400\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1003, 0]"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "labels=['Alex Brush','Titillium Web','Sansation','Open Sans','Ubuntu Mono']\n",
    "print(classification_report(test_y_cat, prediction_arr, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3850 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen_t =  ImageDataGenerator(\n",
    "     rescale=1/255, dtype='float32'\n",
    "     )\n",
    "t_it = datagen_t.flow_from_directory('check_test', batch_size=18, class_mode='categorical',shuffle=False\n",
    "#save_to_dir='augmented',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214/214 [==============================] - 15s 68ms/step - loss: 0.6127 - accuracy: 0.7730\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(t_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214/214 [==============================] - 14s 64ms/step\n"
     ]
    }
   ],
   "source": [
    "predict = model.predict(t_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the class with highest probability for each sample\n",
    "y_pred = np.argmax(predict, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Alex Brush       0.88      0.91      0.90       818\n",
      "Titillium Web       0.55      0.60      0.57       547\n",
      "    Sansation       0.74      0.72      0.73       910\n",
      "    Open Sans       0.75      0.61      0.68       732\n",
      "  Ubuntu Mono       0.86      0.95      0.90       843\n",
      "\n",
      "     accuracy                           0.77      3850\n",
      "    macro avg       0.76      0.76      0.76      3850\n",
      " weighted avg       0.77      0.77      0.77      3850\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(t_it.classes, y_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction_arr_cat = np_utils.to_categorical(prediction_arr, 5)\n",
    "# test_y_cat = np_utils.to_categorical(test_y, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# m = tf.keras.metrics.AUC()\n",
    "\n",
    "# m.update_state(test_y, prediction_arr) # assuming both have shape (N,)\n",
    "\n",
    "# r = m.result().numpy()\n",
    "\n",
    "# print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true = test_y_cat\n",
    "# y_pred = prediction_arr\n",
    "# kacc = tf.keras.metrics.Accuracy()\n",
    "# _ = kacc.update_state(y_true, y_pred)\n",
    "# print(f'Keras Accuracy acc: {kacc.result().numpy()*100:.3}')\n",
    "\n",
    "# kbacc = tf.keras.metrics.BinaryAccuracy()\n",
    "# _ = kbacc.update_state(y_true, y_pred)\n",
    "# print(f'Keras BinaryAccuracy acc: {kbacc.result().numpy()*100:.3}')\n",
    "\n",
    "# print(f'SkLearn acc: {accuracy_score(y_true, y_pred)*100:.3}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "017718ff8815c2e28200b0ec15712f9c3f1df2ede3aa8445637a6e1eb80e7347"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
