{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Computer Vision - Project\n",
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import requests\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf\n",
    "import h5py, requests, os\n",
    "import matplotlib.patches as patches\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten, BatchNormalization, ZeroPadding2D, GlobalMaxPooling2D\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Resizing\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ZIP_PATH = \"https://drive.google.com/drive/folders/1jzHYpTwywUYA53nMGHVROSuVO14hEueq?usp=sharing/\"\n",
    "FILE_NAME =\"SynthText_train.h5\"\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "  tf.config.experimental.set_memory_growth(gpu, True)\n",
    "tf.keras.backend.clear_session()\n",
    "SIZE=224\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_points(points, center):\n",
    "    # calculate the angle of each point from the center point\n",
    "    angles = np.arctan2(points[:, 1] - center[1], points[:, 0] - center[0])\n",
    "    # sort the points by angle\n",
    "    sorted_points = points[np.argsort(angles)]\n",
    "    return sorted_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_training_curve(history):\n",
    "\t\"\"\"\n",
    "\tDraw training curve\n",
    "\tParameters:\n",
    "\t\thistory - contains loss and accuracy from training\n",
    "\tReturns:\n",
    "\t\tNone\n",
    "\t\"\"\"\n",
    "\tplt.figure(1)\n",
    "\n",
    "\t# History for accuracy\n",
    "\tplt.subplot(211)\n",
    "\tplt.plot(history.history['accuracy'])\n",
    "\tplt.plot(history.history['val_accuracy'])\n",
    "\tplt.title('model accuracy')\n",
    "\tplt.ylabel('accuracy')\n",
    "\tplt.xlabel('epoch')\n",
    "\tplt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "\t# History for loss\n",
    "\tplt.subplot(212)\n",
    "\tplt.plot(history.history['loss'])\n",
    "\tplt.plot(history.history['val_loss'])\n",
    "\tplt.title('model loss')\n",
    "\tplt.ylabel('loss')\n",
    "\tplt.xlabel('epoch')\n",
    "\tplt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_points(image, points, color=(255, 0, 0), radius=3):\n",
    "    # create a copy of the image\n",
    "    img = image.copy()\n",
    "    # iterate over the points and draw them on the image\n",
    "    for point in points:\n",
    "        cv2.circle(img, tuple(map(int, point)), radius, color, -1)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def font_to_num(font):\n",
    "    if font == b'Alex Brush':\n",
    "        return 0\n",
    "    elif font == b'Open Sans':\n",
    "        return 1\n",
    "    elif font == b'Sansation':\n",
    "        return 2\n",
    "    elif font == b'Titillium Web':\n",
    "        return 3\n",
    "    else:\n",
    "        return 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_to_font(font):\n",
    "    if font == 0:\n",
    "        return b'Alex Brush'\n",
    "    elif font == 1:\n",
    "        return b'Titillium Web'\n",
    "    elif font == 2:\n",
    "        return b'Sansation'\n",
    "    elif font == 3:\n",
    "        return b'Open Sans'\n",
    "    else:\n",
    "        return b'Ubuntu Mono'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_label(set, index):\n",
    "    line = set[index]\n",
    "    max = np.argmax(line)\n",
    "    print(num_to_font(max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_font(max):\n",
    "     print(num_to_font(max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_photo_from_set(set_x, set_y, index, font):\n",
    "    plt.imshow(set_x[index], cmap='gray')\n",
    "    plt.show()\n",
    "    print_font(font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_img(img, bbs, index, size):\n",
    "    x1 = int(bbs[0,0,index])\n",
    "    y1 = int(bbs[1,0,index])\n",
    "    x2 = int(bbs[0,1,index])\n",
    "    y2 = int(bbs[1,1,index])\n",
    "    x3 = int(bbs[0,2,index])\n",
    "    y3 = int(bbs[1,2,index])\n",
    "    x4 = int(bbs[0,3,index])\n",
    "    y4 = int(bbs[1,3,index])\n",
    "    # calculate bounding rectangle\n",
    "    top_left_x = max(0, min([x1,x2,x3,x4]))\n",
    "    top_left_y = max(0, min([y1,y2,y3,y4]))\n",
    "    bot_right_x = max(0, max([x1,x2,x3,x4]))\n",
    "    bot_right_y = max(0, max([y1,y2,y3,y4]))\n",
    "\n",
    "    cropped = img[top_left_y:bot_right_y+1, top_left_x:bot_right_x+1]\n",
    "    # _, cropped = cv2.threshold(cropped,127,255,cv2.THRESH_TRIANGLE)\n",
    "    # print(cropped.shape)\n",
    "    cropped = tf.image.resize(cropped, (size, size), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    # cropped = tf.image.rgb_to_grayscale(cropped)\n",
    "    # # print(cropped.shape)\n",
    "    cropped = tf.image.convert_image_dtype(cropped, tf.float32)\n",
    "    # plt.imshow(cropped, cmap='gray')\n",
    "    # plt.show()\n",
    "    return cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def create_dirs(main_directory):\n",
    "    Path(main_directory).mkdir(parents=True, exist_ok=True)\n",
    "    Path(main_directory+'/Alex Brush').mkdir(parents=True, exist_ok=True)\n",
    "    Path(main_directory+'/Titillium Web').mkdir(parents=True, exist_ok=True)\n",
    "    Path(main_directory+'/Sansation').mkdir(parents=True, exist_ok=True)\n",
    "    Path(main_directory+'/Open Sans').mkdir(parents=True, exist_ok=True)\n",
    "    Path(main_directory+'/Ubuntu Mono').mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_data(db, im):\n",
    "    img  = db['data'][im][:]\n",
    "    fonts = db['data'][im].attrs['font']\n",
    "    txts = db['data'][im].attrs['txt']\n",
    "    charBBs = db['data'][im].attrs['charBB']\n",
    "    wordBBs = db['data'][im].attrs['wordBB']\n",
    "    return img, fonts, txts, charBBs, wordBBs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_num_or_letter(inp):\n",
    "    # res= ((inp >= ord('a') and inp <= ord('z')) or (inp >= ord('A') and inp <= ord('Z'))) or (inp>=ord('0') and inp<=ord('9'))\n",
    "    # # print(chr(inp)+\"=\"+str(res))\n",
    "    if(inp==ord('.') or inp==ord(',') or inp==ord(':') or inp==ord(';')):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_and_save(img, BBs, indx, size, curr_font, im, num, append_not_save=False, folder='main_directory/'):\n",
    "    cropped = prepare_img(img, BBs, indx, size)\n",
    "    path = folder+curr_font.decode('UTF-8')+'/'+im+'_'+str(num)+'.jpg' \n",
    "    if not append_not_save:\n",
    "        tf.keras.utils.save_img(path,cropped)\n",
    "    return cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = h5py.File(FILE_NAME, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_set(db, size: int):   \n",
    "    create_dirs('main_directory')\n",
    "    im_names = list(db['data'].keys())\n",
    "    num = 0\n",
    "    for i in tqdm(range(0, len(im_names)-1)):\n",
    "        im = im_names[i]\n",
    "        img, fonts, txts, charBBs, wordBBs = get_image_data(db, im)\n",
    "        font_indx = 0 \n",
    "        char_indx = 0\n",
    "        # print(im)\n",
    "        for j in range(0, len(txts)):\n",
    "            if(j%8!=0):\n",
    "                crop_and_save(img, wordBBs, j, size, fonts, font_indx, im, num)\n",
    "                num+=1\n",
    "                for k in range(0, len(txts[j])):\n",
    "                    if(is_num_or_letter(txts[j][k])):\n",
    "                        crop_and_save(img, charBBs, char_indx, size, fonts, font_indx, im, num)\n",
    "                        num+=1\n",
    "                    char_indx+=1\n",
    "            else:\n",
    "                char_indx+=len(txts[j])\n",
    "            font_indx += len(txts[j])\n",
    "    print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from download_h5py_db import download_h5py_db\n",
    "if not Path(\"main_directory\").exists():\n",
    "    download_h5py_db()\n",
    "    get_data_set(db, SIZE)\n",
    "# train_x, train_y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 29840 files [00:51, 584.48 files/s]\n"
     ]
    }
   ],
   "source": [
    "import splitfolders\n",
    "splitfolders.ratio(\"main_directory/\", # The location of dataset\n",
    "                   output=\"main_directory_splitted\", # The output location\n",
    "                   seed=42, # The number of seed\n",
    "                   ratio=(.8, .1, .1), # The ratio of splited dataset\n",
    "                   group_prefix=None, # If your dataset contains more than one file like \".jpg\", \".pdf\", etc\n",
    "                   move=False # If you choose to move, turn this into True\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "def data_augmentation():\n",
    "    datagen =  ImageDataGenerator(\n",
    "        horizontal_flip=True, rotation_range=90, fill_mode='reflect', channel_shift_range=0.8,#\n",
    "         shear_range=15,vertical_flip=False, brightness_range=(0.2, 0.8),# \n",
    "     rescale=1/255, dtype='float32'\n",
    "     #,validation_split=0.25\n",
    "     )\n",
    "    # Path('augmented').mkdir(exist_ok=True)\n",
    "    it = datagen.flow_from_directory('main_directory_splitted/train/', batch_size=18, class_mode='categorical',\n",
    "    #save_to_dir='augmented',\n",
    "     shuffle=True, seed=1, keep_aspect_ratio=True)\n",
    "    datagen_val =  ImageDataGenerator(\n",
    "     rescale=1/255, dtype='float32'\n",
    "     )\n",
    "    val_it = datagen_val.flow_from_directory('main_directory_splitted/val/', batch_size=18, class_mode='categorical',\n",
    "    #save_to_dir='augmented',\n",
    "     shuffle=True)\n",
    "    test_it = datagen_val.flow_from_directory('main_directory_splitted/test/', batch_size=18, class_mode='categorical',\n",
    "    #save_to_dir='augmented',\n",
    "     shuffle=True)\n",
    "    return it, val_it,test_it, datagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23869 images belonging to 5 classes.\n",
      "Found 2981 images belonging to 5 classes.\n",
      "Found 2990 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "it, val_it,test_it, datagen = data_augmentation()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Found 27011 images belonging to 5 classes.\n",
    "Found 9000 images belonging to 5 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datagen.fit(it, augment=True, seed=0.8, rounds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import GaussianNoise\n",
    "\n",
    "from keras import regularizers\n",
    "\n",
    "num_classes = 5\n",
    "input_size= SIZE\n",
    "\n",
    "baseModel = tf.keras.applications.ResNet50(include_top=False, classes=num_classes,\n",
    "                         input_shape=(input_size, input_size, 3),\n",
    "                        weights='imagenet')\n",
    "headModel = baseModel.output\n",
    "headModel = GaussianNoise(0.1)(headModel)\n",
    "headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n",
    "headModel = Flatten(name=\"flatten\")(headModel)\n",
    "headModel = Dropout(0.5)(headModel)\n",
    "headModel = Dense(256, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01))(headModel)\n",
    "headModel = Dropout(0.5)(headModel)\n",
    "headModel = Dense(num_classes, activation=\"softmax\")(headModel)\n",
    "model = Model(inputs=baseModel.input, outputs=headModel)    \n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.00003), loss='categorical_crossentropy', metrics=[tf.keras.metrics.Precision()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    # if epoch < 5:\n",
    "    #     return lr\n",
    "    # else:\n",
    "    return lr * 0.1 #tf.math.exp(-0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback = tf.keras.callbacks.LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(X_train, Y_train, batch_size=16, epochs=20)\n",
    "# datagen.fit(X_train)\n",
    "if not os.path.isfile('model_res.h5'):\n",
    "  history = model.fit(it, epochs=15, shuffle=True, validation_data=val_it, verbose=1\n",
    "          #   ,callbacks=[callback]\n",
    "            )\n",
    "else:\n",
    "  model = tf.keras.models.load_model('model_res.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167/167 [==============================] - 13s 71ms/step - loss: 0.4731 - accuracy: 0.8368\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.4731270968914032 / Test accuracy: 0.8367893099784851\n"
     ]
    }
   ],
   "source": [
    "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[158], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m draw_training_curve(history)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "draw_training_curve(history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_res\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_res\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('model_res')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 249/249 [00:21<00:00, 11.44it/s]\n"
     ]
    }
   ],
   "source": [
    "create_dirs('test_set')\n",
    "size=SIZE\n",
    "im_names = list(db['data'].keys())\n",
    "num = 0\n",
    "prediction_arr=[]\n",
    "test_y = []\n",
    "\n",
    "for i in tqdm(range(0, int((len(im_names)-1)/4))):\n",
    "    im = im_names[i]\n",
    "    img, fonts, txts, charBBs, wordBBs = get_image_data(db, im)\n",
    "    font_indx = 0 \n",
    "    char_indx = 0\n",
    "    curr_font = fonts[font_indx]\n",
    "    # print(im)\n",
    "    for j in range(0, int(len(txts)/4)):\n",
    "        if(j%8==0): \n",
    "            test_x = [] \n",
    "            cropped = crop_and_save(img, wordBBs, j, size, curr_font, im, num, False, 'test_set/')\n",
    "            test_x.append(cropped)\n",
    "            # test_y.append(font_to_num(curr_font))\n",
    "            num+=1            \n",
    "            for k in range(0, len(txts[j])):\n",
    "                if(is_num_or_letter(txts[j][k])):\n",
    "                    cropped = crop_and_save(img, charBBs, char_indx, size, curr_font, im, num, False, 'test_set/')\n",
    "                    test_x.append(cropped)\n",
    "                    # test_y.append(font_to_num(curr_font))\n",
    "                    num+=1\n",
    "                char_indx+=1\n",
    "            test_x = np.asarray(test_x, dtype=np.float32)\n",
    "            # # print(test_x)\n",
    "            reses = model.predict(test_x, verbose=0)\n",
    "            # # print(reses)\n",
    "            maxes = np.argmax(reses, axis=1)\n",
    "            prediction = np.bincount(maxes).argmax()\n",
    "            for k in range(0, len(txts[j])):\n",
    "                test_y.append(font_to_num(fonts[font_indx]))\n",
    "                prediction_arr.append(prediction)\n",
    "            # if(fonts[font_indx]==b'Open Sans'):\n",
    "            # print(txts[j])\n",
    "            # for imi in test_x:\n",
    "            #     plt.imshow(imi)\n",
    "            #     plt.show()\n",
    "            # print(maxes)\n",
    "            # print(reses)\n",
    "            # print(prediction)\n",
    "            # print(font_to_num(fonts[font_indx]))\n",
    "            # print(num_to_font(prediction))\n",
    "            # print(test_x)\n",
    "            # print(str(np.bincount(maxes).argmax())+\"-\"+str(font_to_num(fonts[font_indx])))\n",
    "            # print(np.bincount(maxes).argmax())\n",
    "        else:\n",
    "            char_indx+=len(txts[j])\n",
    "        font_indx += len(txts[j])\n",
    "# print(len(test_x))\n",
    "# print(len(test_y))\n",
    "# test_x = np.asarray(test_x, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Alex Brush       0.86      1.00      0.93       169\n",
      "    Open Sans       0.76      0.86      0.81       142\n",
      "Titillium Web       0.84      0.85      0.84       208\n",
      "    Sansation       0.88      0.72      0.79       158\n",
      "  Ubuntu Mono       0.93      0.82      0.87       160\n",
      "\n",
      "     accuracy                           0.85       837\n",
      "    macro avg       0.85      0.85      0.85       837\n",
      " weighted avg       0.85      0.85      0.85       837\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "labels=['Alex Brush','Open Sans','Titillium Web','Sansation','Ubuntu Mono']\n",
    "print(classification_report(test_y, prediction_arr, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1003\n"
     ]
    }
   ],
   "source": [
    "print(len(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 4s 105ms/step\n"
     ]
    }
   ],
   "source": [
    "prediction_arr = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=np.argmax(prediction_arr, axis=1)\n",
    "# y_test=np.argmax(test_y_cat, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = np.asarray(test_y, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_cat = np_utils.to_categorical(test_y, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1003,)\n"
     ]
    }
   ],
   "source": [
    "# print(test_y)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "labels=['Alex Brush','Open Sans','Titillium Web','Sansation','Ubuntu Mono']\n",
    "print(classification_report(test_y, prediction_arr, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for im in test_x:\n",
    "#     tf.keras.utils.save_img('path',cropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen_try =  ImageDataGenerator(\n",
    "     rescale=1/255, dtype='float32',\n",
    "     )\n",
    "it_try = datagen_try.flow(test_x,y_pred, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 3s 107ms/step\n"
     ]
    }
   ],
   "source": [
    "pted_try = model.predict(it_try)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=np.argmax(prediction_arr, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Alex Brush       0.80      0.93      0.86       204\n",
      "Titillium Web       0.14      0.12      0.13       195\n",
      "    Sansation       0.73      0.68      0.70       248\n",
      "    Open Sans       0.13      0.14      0.13       168\n",
      "  Ubuntu Mono       0.81      0.82      0.81       188\n",
      "\n",
      "     accuracy                           0.56      1003\n",
      "    macro avg       0.52      0.54      0.53      1003\n",
      " weighted avg       0.54      0.56      0.55      1003\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_y, y_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3850 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen_t =  ImageDataGenerator(\n",
    "     rescale=1/255, dtype='float32'\n",
    "     )\n",
    "t_it = datagen_t.flow_from_directory('check_test', batch_size=18, class_mode='categorical',shuffle=False\n",
    "#save_to_dir='augmented',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214/214 [==============================] - 15s 68ms/step - loss: 0.6127 - accuracy: 0.7730\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(t_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214/214 [==============================] - 14s 64ms/step\n"
     ]
    }
   ],
   "source": [
    "predict = model.predict(t_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the class with highest probability for each sample\n",
    "y_pred = np.argmax(predict, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Alex Brush       0.88      0.91      0.90       818\n",
      "Titillium Web       0.55      0.60      0.57       547\n",
      "    Sansation       0.74      0.72      0.73       910\n",
      "    Open Sans       0.75      0.61      0.68       732\n",
      "  Ubuntu Mono       0.86      0.95      0.90       843\n",
      "\n",
      "     accuracy                           0.77      3850\n",
      "    macro avg       0.76      0.76      0.76      3850\n",
      " weighted avg       0.77      0.77      0.77      3850\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(t_it.classes, y_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction_arr_cat = np_utils.to_categorical(prediction_arr, 5)\n",
    "# test_y_cat = np_utils.to_categorical(test_y, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# m = tf.keras.metrics.AUC()\n",
    "\n",
    "# m.update_state(test_y, prediction_arr) # assuming both have shape (N,)\n",
    "\n",
    "# r = m.result().numpy()\n",
    "\n",
    "# print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true = test_y_cat\n",
    "# y_pred = prediction_arr\n",
    "# kacc = tf.keras.metrics.Accuracy()\n",
    "# _ = kacc.update_state(y_true, y_pred)\n",
    "# print(f'Keras Accuracy acc: {kacc.result().numpy()*100:.3}')\n",
    "\n",
    "# kbacc = tf.keras.metrics.BinaryAccuracy()\n",
    "# _ = kbacc.update_state(y_true, y_pred)\n",
    "# print(f'Keras BinaryAccuracy acc: {kbacc.result().numpy()*100:.3}')\n",
    "\n",
    "# print(f'SkLearn acc: {accuracy_score(y_true, y_pred)*100:.3}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "017718ff8815c2e28200b0ec15712f9c3f1df2ede3aa8445637a6e1eb80e7347"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
