{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\simon\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1769: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten, BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import splitfolders\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import GaussianNoise\n",
    "from keras import regularizers\n",
    "from download_h5py_db import download_h5py_db\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import load_model\n",
    "from pathlib import Path\n",
    "from utils import show_results\n",
    "from h5py import File\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = 'test_data.h5'\n",
    "path = 'res//res.h5'\n",
    "SIZE=224\n",
    "num_classes = 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def font_to_num(font):\n",
    "    if font == b'Alex Brush':\n",
    "        return 0\n",
    "    elif font == b'Open Sans':\n",
    "        return 1\n",
    "    elif font == b'Sansation':\n",
    "        return 2\n",
    "    elif font == b'Titillium Web':\n",
    "        return 3\n",
    "    else:\n",
    "        return 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_to_font(font):\n",
    "    if font == 0:\n",
    "        return b'Alex Brush'\n",
    "    elif font == 1:\n",
    "        return b'Open Sans'\n",
    "    elif font == 2:\n",
    "        return b'Sansation'\n",
    "    elif font == 3:\n",
    "        return b'Titillium Web'\n",
    "    else:\n",
    "        return b'Ubuntu Mono'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_img(img, bbs, index, size = SIZE):\n",
    "    x1 = int(bbs[0,0,index])\n",
    "    y1 = int(bbs[1,0,index])\n",
    "    x2 = int(bbs[0,1,index])\n",
    "    y2 = int(bbs[1,1,index])\n",
    "    x3 = int(bbs[0,2,index])\n",
    "    y3 = int(bbs[1,2,index])\n",
    "    x4 = int(bbs[0,3,index])\n",
    "    y4 = int(bbs[1,3,index])\n",
    "    # calculate bounding rectangle\n",
    "    top_left_x = max(0, min([x1,x2,x3,x4]))\n",
    "    top_left_y = max(0, min([y1,y2,y3,y4]))\n",
    "    bot_right_x = max(0, max([x1,x2,x3,x4]))\n",
    "    bot_right_y = max(0, max([y1,y2,y3,y4]))\n",
    "\n",
    "    cropped = img[top_left_y:bot_right_y+1, top_left_x:bot_right_x+1]\n",
    "    cropped = tf.image.resize(cropped, (size, size), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    cropped = tf.image.convert_image_dtype(cropped, tf.float32)\n",
    "    return cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dirs(main_dir):\n",
    "    Path(main_dir).mkdir(parents=True, exist_ok=True)\n",
    "    Path(main_dir+'/Alex Brush').mkdir(parents=True, exist_ok=True)\n",
    "    Path(main_dir+'/Titillium Web').mkdir(parents=True, exist_ok=True)\n",
    "    Path(main_dir+'/Sansation').mkdir(parents=True, exist_ok=True)\n",
    "    Path(main_dir+'/Open Sans').mkdir(parents=True, exist_ok=True)\n",
    "    Path(main_dir+'/Ubuntu Mono').mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_data(db, im, has_labels=True):\n",
    "    img  = db['data'][im][:]\n",
    "    if has_labels:\n",
    "        fonts = db['data'][im].attrs['font']  \n",
    "    else: \n",
    "        fonts = None\n",
    "    txts = db['data'][im].attrs['txt']\n",
    "    charBBs = db['data'][im].attrs['charBB']\n",
    "    wordBBs = db['data'][im].attrs['wordBB']\n",
    "    return img, fonts, txts, charBBs, wordBBs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_not_dot(inp):\n",
    "    res= (inp != ord('.') and inp != ord(':'))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_and_save(img, BBs, indx, size, curr_font, im, num, append_not_save=False, folder='main_directory/'):\n",
    "    cropped = prepare_img(img, BBs, indx, size)\n",
    "    if not append_not_save:\n",
    "        path = folder+curr_font.decode('UTF-8')+'/'+im+'_'+str(num)+'.png'\n",
    "        tf.keras.utils.save_img(path,cropped)\n",
    "    return cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from utils import create_dirs, SIZE, get_image_data, crop_and_save, font_to_num\n",
    "from numpy import asarray, argmax, bincount,argwhere\n",
    "from h5py import File\n",
    "from sklearn.preprocessing import normalize\n",
    "import csv\n",
    "from h5py import File\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "\n",
    "\n",
    "class Font_classifier:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def make_row(self, index, im, ch, prediction, writer):\n",
    "        a = []\n",
    "        a.append(index)\n",
    "        a.append(im)\n",
    "        a.append(ch)\n",
    "        b = np_utils.to_categorical(prediction, 5)\n",
    "        # print(prediction)\n",
    "        # print(a)\n",
    "        # print(b)\n",
    "        b = np.concatenate((a,b[1:],[b[0]]))\n",
    "        writer.writerow(b)\n",
    "        index+=1\n",
    "\n",
    "    def predict(self, db_file):\n",
    "        db = File(db_file, 'r')   \n",
    "        size=SIZE\n",
    "        im_names = list(db['data'].keys())\n",
    "        num = 0\n",
    "        index = 0\n",
    "        # prediction_arr=[]\n",
    "        # images = []\n",
    "        # chars=[]\n",
    "        with open('test_lables.csv', 'w+', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\" \", \"image\", \"char\",\"Open Sans\",\"Sansation\",\"Titillium Web\",\"Ubuntu Mono\",\"Alex Brush\"])\n",
    "            for i in tqdm(range(0, len(im_names))):\n",
    "                im = im_names[i]\n",
    "                img, _, txts, charBBs, wordBBs = get_image_data(db, im, has_labels=False)\n",
    "                font_indx = 0 \n",
    "                char_indx = 0\n",
    "                for j in range(0, len(txts)):\n",
    "                    test_x = [] \n",
    "                    cropped = crop_and_save(img, wordBBs, j, size, None, im, num, True)\n",
    "                    test_x.append(cropped)\n",
    "                    num+=1            \n",
    "                    for k in range(0, len(txts[j])):\n",
    "                        word =  txts[j]\n",
    "                        cropped = crop_and_save(img, charBBs, char_indx, size, None, im, num, True)\n",
    "                        test_x.append(cropped)\n",
    "                        num+=1\n",
    "                        # char_indx+=1\n",
    "                        index += 1\n",
    "                    test_x = np.asarray(test_x, dtype=np.float32)\n",
    "                    reses = self.model.predict(test_x, verbose=0)\n",
    "                    maxes = np.argmax(reses, axis=1)\n",
    "                    prediction = np.bincount(maxes)\n",
    "                    prediction = np.argwhere(prediction==prediction.max())\n",
    "                    if (len(prediction)>1):\n",
    "                        reses_n = np.normalize(reses, axis=1, norm='l1')\n",
    "                        maxes_n = np.argmax(reses_n, axis=1)\n",
    "                        prediction = np.bincount(maxes_n)\n",
    "                        prediction = np.argwhere(prediction==prediction.max())\n",
    "                        if(len(prediction)>1):\n",
    "                            sum_p = np.sum(reses, axis=0)\n",
    "                            prediction = sum_p.argmax()\n",
    "                    for k in range(0, len(word)):\n",
    "                        a = []\n",
    "                        a.append(index)\n",
    "                        a.append(im)\n",
    "                        a.append(chr(word[k]))\n",
    "                        b = np_utils.to_categorical(prediction.item(), 5)\n",
    "                        b = np.concatenate((a,b[1:],[b[0]]))\n",
    "                        # print(b)\n",
    "                        writer.writerow(b)\n",
    "                        index+=1\n",
    "                        # prediction_arr.append(prediction.item())\n",
    "                        # images.append(im)\n",
    "                    font_indx += len(txts[j])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = 'test_data.h5'\n",
    "if not Path('test_data.h5').exists():\n",
    "    download_file_from_google_drive('1YwLcXqLArFSOtoepQw7nC1t4jC8CFxpI', FILE_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model_, db_file):\n",
    "        db = File(db_file, 'r')   \n",
    "        size=SIZE\n",
    "        im_names = list(db['data'].keys())\n",
    "        num = 0\n",
    "        index = 0\n",
    "        # prediction_arr=[]\n",
    "        # images = []\n",
    "        # chars=[]\n",
    "        with open('test_lables.csv', 'w+', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\" \", \"image\", \"char\",\"Open Sans\",\"Sansation\",\"Titillium Web\",\"Ubuntu Mono\",\"Alex Brush\"])\n",
    "            for i in tqdm(range(0, len(im_names))):\n",
    "                im = im_names[i]\n",
    "                img, _, txts, charBBs, wordBBs = get_image_data(db, im, has_labels=False)\n",
    "                font_indx = 0 \n",
    "                char_indx = 0\n",
    "                for j in range(0, len(txts)):\n",
    "                    test_x = [] \n",
    "                    cropped = crop_and_save(img, wordBBs, j, size, None, im, num, True)\n",
    "                    test_x.append(cropped)\n",
    "                    num+=1            \n",
    "                    for k in range(0, len(txts[j])):\n",
    "                        word =  txts[j]\n",
    "                        cropped = crop_and_save(img, charBBs, char_indx, size, None, im, num, True)\n",
    "                        test_x.append(cropped)\n",
    "                        num+=1\n",
    "                        # char_indx+=1\n",
    "                        index += 1\n",
    "                    test_x = np.asarray(test_x, dtype=np.float32)\n",
    "                    reses = model_.predict(test_x, verbose=0)\n",
    "                    maxes = np.argmax(reses, axis=1)\n",
    "                    prediction = np.bincount(maxes)\n",
    "                    prediction = np.argwhere(prediction==prediction.max())\n",
    "                    if (len(prediction)>1):\n",
    "                        reses_n = np.normalize(reses, axis=1, norm='l1')\n",
    "                        maxes_n = np.argmax(reses_n, axis=1)\n",
    "                        prediction = np.bincount(maxes_n)\n",
    "                        prediction = np.argwhere(prediction==prediction.max())\n",
    "                        if(len(prediction)>1):\n",
    "                            sum_p = np.sum(reses, axis=0)\n",
    "                            prediction = sum_p.argmax()\n",
    "                    for k in range(0, len(word)):\n",
    "                        a = []\n",
    "                        a.append(index)\n",
    "                        a.append(im)\n",
    "                        a.append(chr(word[k]))\n",
    "                        b = np_utils.to_categorical(prediction.item(), 5)\n",
    "                        b = np.concatenate((a,b[1:],[b[0]]))\n",
    "                        # print(b)\n",
    "                        writer.writerow(b)\n",
    "                        index+=1\n",
    "                        # prediction_arr.append(prediction.item())\n",
    "                        # images.append(im)\n",
    "                    font_indx += len(txts[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/1004 [04:05<13:36:27, 49.04s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pred_y \u001b[39m=\u001b[39m predict(model, FILE_NAME)\n",
      "Cell \u001b[1;32mIn[38], line 31\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(model_, db_file)\u001b[0m\n\u001b[0;32m     29\u001b[0m     index \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     30\u001b[0m test_x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(test_x, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m---> 31\u001b[0m reses \u001b[39m=\u001b[39m model_\u001b[39m.\u001b[39;49mpredict(test_x, verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m     32\u001b[0m maxes \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(reses, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     33\u001b[0m prediction \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mbincount(maxes)\n",
      "File \u001b[1;32mc:\\Users\\simon\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\simon\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\training.py:2253\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2251\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39msteps():\n\u001b[0;32m   2252\u001b[0m     callbacks\u001b[39m.\u001b[39mon_predict_batch_begin(step)\n\u001b[1;32m-> 2253\u001b[0m     tmp_batch_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_function(iterator)\n\u001b[0;32m   2254\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   2255\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\simon\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\simon\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\simon\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:954\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    952\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    953\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 954\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    955\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[0;32m    956\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    957\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\simon\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\simon\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\simon\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\simon\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pred_y = predict(model, FILE_NAME)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "017718ff8815c2e28200b0ec15712f9c3f1df2ede3aa8445637a6e1eb80e7347"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
